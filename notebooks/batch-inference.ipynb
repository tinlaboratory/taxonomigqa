{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch inference with Gemma/PaliGemma with HF + GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models, developed by Google DeepMind and other teams across Google. Text Generation Inference (TGI) is a toolkit developed by Hugging Face for deploying and serving LLMs, with high performance text generation. And, Google Vertex AI is a Machine Learning (ML) platform that lets you train and deploy ML models and AI applications, and customize large language models (LLMs) for use in your AI-powered applications. This example showcases how to deploy any supported text-generation model, in this case [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it), from the Hugging Face Hub on Vertex AI using the TGI DLC available in Google Cloud Platform (GCP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![`google/gemma-7b-it` in the Hugging Face Hub](./assets/model-in-hf-hub.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup / Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to install `gcloud` in your local machine, which is the command-line tool for Google Cloud, following the instructions at [Cloud SDK Documentation - Install the gcloud CLI](https://cloud.google.com/sdk/docs/install)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you also need to install the `google-cloud-aiplatform` Python SDK, required to programmatically create the Vertex AI model, register it, acreate the endpoint, and deploy it on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (1.70.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.19.2)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (2.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (1.24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (5.28.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (24.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (2.18.2)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (1.12.5)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: docstring-parser<1 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.32.3)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.66.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.6.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from pydantic<3->google-cloud-aiplatform) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (2.0.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dali/work/postdoc/multimodal-semantic-signals/semantic-signals-env/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2024.7.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, to ease the usage of the commands within this tutorial, you need to set the following environment variables for GCP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=multimodal-representations\n",
      "env: LOCATION=us-central1\n",
      "env: CONTAINER_URI=us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310:latest\n"
     ]
    }
   ],
   "source": [
    "%env PROJECT_ID=multimodal-representations\n",
    "%env LOCATION=us-central1\n",
    "%env CONTAINER_URI=us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to login into your GCP account and set the project ID to the one you want to use to register and deploy the models on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=SVZIOf0TwZijRq9vSPjOY4pYUZeVWy&access_type=offline&code_challenge=oOEdflrwYAWtdPpMJ7vZZWXkt6Ro1wXJJisvyihidJc&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [daliumuwork@gmail.com].\n",
      "Your current project is [multimodal-representations].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are logged in, you need to enable the necessary service APIs in GCP, such as the Vertex AI API, the Compute Engine API, and Google Container Registry related APIs.\n",
    "\n",
    "**Warning:** Make sure, manually, that these are disabled after running exps (even though we will explicitly write code to disable them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation \"operations/acf.p2-841337720906-482ed210-3ed3-4346-af2c-66d0628a0a49\" finished successfully.\n",
      "Operation \"operations/acat.p2-841337720906-20969f9c-87d2-478f-ab22-7a2ff3ed73f3\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com\n",
    "!gcloud services enable compute.googleapis.com\n",
    "!gcloud services enable container.googleapis.com\n",
    "!gcloud services enable containerregistry.googleapis.com\n",
    "!gcloud services enable containerfilesystem.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is set up, you can already initialize the Vertex AI session via the `google-cloud-aiplatform` Python SDK as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=os.getenv(\"PROJECT_ID\"),\n",
    "    location=os.getenv(\"LOCATION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Gemma models are gated, you need to login into your Hugging Face Hub account with a read-access token either fine-grained with access to the gated model, or just overall read-access to your account. More information on how to generate a read-only access token for the Hugging Face Hub in the instructions at <https://huggingface.co/docs/hub/en/security-tokens>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your token (input will not be visible):  ········\n",
      "Add token as git credential? (Y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/dali/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can already \"upload\" the model i.e. register the model on Vertex AI. It is not an upload per se, since the model will be automatically downloaded from the Hugging Face Hub in the Hugging Face DLC for TGI on startup via the `MODEL_ID` environment variable, so what is uploaded is only the configuration, not the model weights.\n",
    "\n",
    "Before going into the code, let's quickly review the arguments provided to the `upload` method:\n",
    "\n",
    "* **`display_name`** is the name that will be shown in the Vertex AI Model Registry.\n",
    "\n",
    "* **`serving_container_image_uri`** is the location of the Hugging Face DLC for TGI that will be used for serving the model.\n",
    "\n",
    "* **`serving_container_environment_variables`** are the environment variables that will be used during the container runtime, so these are aligned with the environment variables defined by `text-generation-inference`, which are analog to the [`text-generation-launcher` arguments](https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/launcher). Additionally, the Hugging Face DLCs for TGI also capture the `AIP_` environment variables from Vertex AI as in [Vertex AI Documentation - Custom container requirements for prediction](https://cloud.google.com/vertex-ai/docs/predictions/custom-container-requirements).\n",
    "\n",
    "    * `MODEL_ID` is the identifier of the model in the Hugging Face Hub. To explore all the supported models you can check <https://huggingface.co/models?sort=trending&other=text-generation-inference>.\n",
    "    * `NUM_SHARD` is the number of shards to use if you don't want to use all GPUs on a given machine e.g. if you have two GPUs but you just want to use one for TGI then `NUM_SHARD=1`, otherwise it matches the `CUDA_VISIBLE_DEVICES`.\n",
    "    * `MAX_INPUT_TOKENS` is the maximum allowed input length (expressed in number of tokens), the larger it is, the larger the prompt can be, but also more memory will be consumed.\n",
    "    * `MAX_TOTAL_TOKENS` is the most important value to set as it defines the \"memory budget\" of running clients requests, the larger this value, the larger amount each request will be in your RAM and the less effective batching can be.\n",
    "    * `MAX_BATCH_PREFILL_TOKENS` limits the number of tokens for the prefill operation, as it takes the most memory and is compute bound, it is interesting to limit the number of requests that can be sent.\n",
    "    * `HUGGING_FACE_HUB_TOKEN` is the Hugging Face Hub token, required as [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) is a gated model.\n",
    "\n",
    "* (optional) **`serving_container_ports`** is the port where the Vertex AI endpoint will be exposed, by default 8080.\n",
    "\n",
    "For more information on the supported `aiplatform.Model.upload` arguments, check its Python reference at https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310:latest\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv(\"CONTAINER_URI\"))\n",
    "#huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/dali/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"us-docker.pkg.dev\": \"gcloud\",\n",
      "    \"gcr.io\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth configure-docker us-docker.pkg.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/841337720906/locations/us-central1/models/1775620019393134592/operations/1035206312268398592\n",
      "Model created. Resource name: projects/841337720906/locations/us-central1/models/1775620019393134592@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/841337720906/locations/us-central1/models/1775620019393134592@1')\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import get_token\n",
    "\n",
    "HF_MODEL = \"google/paligemma-3b-pt-224\" # change this\n",
    "DISPLAY_NAME = \"paligemma-3b-pt-224\"\n",
    "\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=DISPLAY_NAME,\n",
    "    serving_container_image_uri=os.getenv(\"CONTAINER_URI\"),\n",
    "    serving_container_environment_variables={\n",
    "        \"MODEL_ID\": HF_MODEL,\n",
    "        \"NUM_SHARD\": \"1\",\n",
    "        \"MAX_INPUT_TOKENS\": \"512\", # I am sure these can be further optimized\n",
    "        \"MAX_TOTAL_TOKENS\": \"1024\",\n",
    "        \"MAX_BATCH_PREFILL_TOKENS\": \"1512\",\n",
    "        \"HUGGING_FACE_HUB_TOKEN\": get_token(),\n",
    "    },\n",
    "    serving_container_ports=[8080],\n",
    ")\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model on Vertex AI Model Registry](./assets/vertex-ai-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy model on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is registered on Vertex AI, you need to define the endpoint that you want to deploy the model to, and then link the model deployment to that endpoint resource.\n",
    "\n",
    "To do so, you need to call the method `aiplatform.Endpoint.create` to create a new Vertex AI endpoint resource (which is not linked to a model or anything usable yet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/841337720906/locations/us-central1/endpoints/7248140079485419520/operations/6034764848603070464\n",
      "Endpoint created. Resource name: projects/841337720906/locations/us-central1/endpoints/7248140079485419520\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/841337720906/locations/us-central1/endpoints/7248140079485419520')\n"
     ]
    }
   ],
   "source": [
    "endpoint = aiplatform.Endpoint.create(display_name=f\"{DISPLAY_NAME}-endpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Vertex AI Endpoint created](./assets/vertex-ai-endpoint.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can deploy the registered model in an endpoint on Vertex AI.\n",
    "\n",
    "The `deploy` method will link the previously created endpoint resource with the model that contains the configuration of the serving container, and then, it will deploy the model on Vertex AI in the specified instance.\n",
    "\n",
    "Before going into the code, let's quickly review the arguments provided to the `deploy` method:\n",
    "\n",
    "- **`endpoint`** is the endpoint to deploy the model to, which is optional, and by default will be set to the model display name with the `_endpoint` suffix.\n",
    "- **`machine_type`**, **`accelerator_type`** and **`accelerator_count`** are arguments that define which instance to use, and additionally, the accelerator to use and the number of accelerators, respectively. The `machine_type` and the `accelerator_type` are tied together, so you will need to select an instance that supports the accelerator that you are using and vice-versa. More information about the different instances at [Compute Engine Documentation - GPU machine types](https://cloud.google.com/compute/docs/gpus), and about the `accelerator_type` naming at [Vertex AI Documentation - MachineSpec](https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec).\n",
    "\n",
    "For more information on the supported `aiplatform.Model.deploy` arguments, you can check its Python reference at https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.Model#google_cloud_aiplatform_Model_deploy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying model to Endpoint : projects/841337720906/locations/us-central1/endpoints/7248140079485419520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/841337720906/locations/us-central1/endpoints/7248140079485419520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploy Endpoint model backing LRO: projects/841337720906/locations/us-central1/endpoints/7248140079485419520/operations/7278286111338659840\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/841337720906/locations/us-central1/endpoints/7248140079485419520/operations/7278286111338659840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint model deployed. Resource name: projects/841337720906/locations/us-central1/endpoints/7248140079485419520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/841337720906/locations/us-central1/endpoints/7248140079485419520\n"
     ]
    }
   ],
   "source": [
    "deployed_model = model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    machine_type=\"g2-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_L4\",\n",
    "    accelerator_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING**: _The Vertex AI endpoint deployment via the `deploy` method may take from 15 to 25 minutes._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online predictions on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can run the online predictions on Vertex AI using the `predict` method, which will send the requests to the running endpoint in the `/predict` route specified within the container following Vertex AI I/O payload formatting.\n",
    "\n",
    "As you are serving a `text-generation` model, you will need to make sure that the chat template, if any, is applied correctly to the input conversation; meaning that `transformers` need to be installed so as to instantiate the `tokenizer` for [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it) and run the `apply_chat_template` method over the input conversation before sending the input within the payload to the Vertex AI endpoint.\n",
    "\n",
    "**Note:** The chat template might not be needed for Gemma-2b/the model PaliGemma is based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the installation is complete, the following snippet will apply the chat template to the conversation:\n",
    "\n",
    ">**This isn't needed for PaliGemma but might be needed for Gemma-2b/the LM it is based on.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Via Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Within the same session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are willing to run the online prediction within the current session, you can send requests programmatically via the `aiplatform.Endpoint` (returned by the `aiplatform.Model.deploy` method) as in the following snippet:\n",
    "\n",
    "\n",
    "Here, we have listed code to supply images to the model via two different ways: (1) via urls, and (2) from local files. KM has extrapolated this syntax from the TGI docs on huggingface: https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/visual_language_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**URL Based**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/rabbit.png\"\n",
    "\n",
    "output = deployed_model.predict(\n",
    "    instances=[\n",
    "        {\n",
    "            \"inputs\":\"![]{url}What is the animal wearing?\",\n",
    "            \"parameters\":{\"max_new_tokens\": 100, \"do_sample\": False}\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "# > helmet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=['clothes'], deployed_model_id='9198981570415820800', metadata=None, model_version_id='1', model_resource_name='projects/841337720906/locations/us-central1/models/1775620019393134592', explanations=None)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Locally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"fgqa_hs/images/2333016.png\"\n",
    "\n",
    "with open(image_path, \"rb\") as f:\n",
    "    image = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "image = f\"data:image/png;base64,{image}\"\n",
    "\n",
    "output = deployed_model.predict(\n",
    "    instances=[\n",
    "        {\n",
    "            \"inputs\":f\"![]({image})What is the animal wearing?\",\n",
    "            \"parameters\":{\"max_new_tokens\": 100, \"do_sample\": False}\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "#> space suit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(predictions=['suit'], deployed_model_id='9198981570415820800', metadata=None, model_version_id='1', model_resource_name='projects/841337720906/locations/us-central1/models/1775620019393134592', explanations=None)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# is different when image is passed from a local file vs. from an url\n",
    "# even when we use greedy decoding (I think?), strange!\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Producing the following `output`:\n",
    "\n",
    "```\n",
    "Prediction(predictions=['space suit'], deployed_model_id='6484700777808920576', metadata=None, model_version_id='1', model_resource_name='projects/20178026/locations/us-central1/models/1030014796319162368', explanations=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answers(model, images, questions, gen_config):\n",
    "    assert len(images) == len(questions)\n",
    "\n",
    "    instances = []\n",
    "\n",
    "    #generation_config = {\n",
    "    #    \"max_new_tokens\": 256,\n",
    "    #    \"do_sample\": True,\n",
    "    #    \"top_p\": 0.2,\n",
    "    #    \"temperature\": 0.2,\n",
    "    #}\n",
    "\n",
    "    \n",
    "    binarized_images = []\n",
    "    for i, image in enumerate(images):\n",
    "        img = f\"data:image/png;base64,{image}\"\n",
    "        instance = {\"inputs\":f\"![]({img}){questions[i]}\",\n",
    "                \"parameters\": gen_config}\n",
    "        instances.append(instance)\n",
    "    print(len(instances[0][\"inputs\"]))\n",
    "    output = model.predict(\n",
    "        instances=instances\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561586\n",
      "helmet\n",
      "yes y preds threatens alos garden\n",
      "Prediction(predictions=['helmet', 'yes y preds threatens alos garden'], deployed_model_id='9198981570415820800', metadata=None, model_version_id='1', model_resource_name='projects/841337720906/locations/us-central1/models/1775620019393134592', explanations=None)\n"
     ]
    }
   ],
   "source": [
    "image_path = \"/home/dali/Downloads/rabbit.png\"\n",
    "\n",
    "generation_config = {\"max_new_tokens\": 100, \"do_sample\": False}\n",
    "\n",
    "with open(image_path, \"rb\") as f:\n",
    "    image = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    images = [image, image]\n",
    "    output = predict_answers(deployed_model, \n",
    "                             images, \n",
    "                             [\"What is the animal wearing?\", \"Is the animal in the image threatening?\"], \n",
    "                             generation_config)\n",
    "for prediction in output.predictions:\n",
    "    print(prediction)\n",
    "print(output)\n",
    "#> space suit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation task\n",
    "\n",
    "Based on [https://huggingface.co/docs/google-cloud/main/examples/vertex-ai-notebooks-evaluate-llms-with-vertex-ai](https://huggingface.co/docs/google-cloud/main/examples/vertex-ai-notebooks-evaluate-llms-with-vertex-ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'argument', 'image', 'substitutions'],\n",
       "    num_rows: 11701\n",
       "})"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"fgqa_hs\", split='test')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Is the curtain on the right side or on the left of the picture?',\n",
       " 'answer': 'right',\n",
       " 'argument': 'curtain',\n",
       " 'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x334>,\n",
       " 'substitutions': {'hypernym': ['blind',\n",
       "   'protective covering',\n",
       "   'covering',\n",
       "   'cloth'],\n",
       "  'question': ['Is the blind on the right side or on the left of the picture?',\n",
       "   'Is the protective covering on the right side or on the left of the picture?',\n",
       "   'Is the covering on the right side or on the left of the picture?',\n",
       "   'Is the cloth on the right side or on the left of the picture?']}}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must convert to a pandas dataset in order to use the Vertex Evaluation API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bytes': None,\n",
       " 'path': '/home/dali/.cache/huggingface/datasets/downloads/extracted/9d8531eb1664568201094473c8dc315fc8c009a6bb87849aacd669850d60ef87/2333016.jpg'}"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['image'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_image_old(image, img_type='jpg'):\n",
    "    with open(f\"{path_prefix}{img_id}.{img_type}\", \"rb\") as f:\n",
    "        image = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "        return f\"data:image/{img_type};base64,{image}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 4) (376898519.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[234], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    with open(f\"{img_path}, \"rb\") as f:\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from io import BytesIO\n",
    "def binarize_image(img_path, img_type='jpg'):\n",
    "    \n",
    "    with open(f\"{img_path}, \"rb\") as f:\n",
    "        image = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    return f\"data:image/{img_type};base64,{image}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[233], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mbinarize_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[226], line 4\u001b[0m, in \u001b[0;36mbinarize_image\u001b[0;34m(image, img_type)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbinarize_image\u001b[39m(image, img_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m      3\u001b[0m     buff \u001b[38;5;241m=\u001b[39m BytesIO()\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(buff, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJPEG\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     image \u001b[38;5;241m=\u001b[39m base64\u001b[38;5;241m.\u001b[39mb64encode(buff\u001b[38;5;241m.\u001b[39mgetvalue())\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata:image/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;base64,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "binarize_image(df['image'][0]['path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0. Convert images to base64 representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"img\"] = df[\"imageId\"].apply(lambda x: binarize_image(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     ![](data:image/jpg;base64,/9j/4AAQSkZJRgABAQEA...\n",
       "1     ![](data:image/jpg;base64,/9j/4AAQSkZJRgABAQEA...\n",
       "2     ![](data:image/jpg;base64,/9j/4AAQSkZJRgABAQEA...\n",
       "3     ![](data:image/jpg;base64,/9j/4AAQSkZJRgABAQEA...\n",
       "4     ![](data:image/jpg;base64,/9j/4AAQSkZJRgABAQEA...\n",
       "                            ...                        \n",
       "95    ![](data:image/jpg;base64,/9j/4AAQSkZJRgABAQEA...\n",
       "96    ![](data:image/jpg;base64,/9j/4AAQSkZJRgABAQEA...\n",
       "97    ![](data:image/jpg;base64,/9j/4AAQSkZJRgABAQEA...\n",
       "98    ![](data:image/jpg;base64,/9j/4AAQSkZJRgABAQEB...\n",
       "99    ![](data:image/jpg;base64,/9j/4AAQSkZJRgABAQEB...\n",
       "Name: prompt, Length: 100, dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['prompt'] = df.apply(lambda row: f\"![]({row['img']})Answer the following question about the given image. Answer with a single concept: {row['question']}\", axis=1)\n",
    "df['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reference'] = df['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all columns that we do not need for the prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['prompt','reference']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.evaluation import EvalTask\n",
    "# 2. create eval task\n",
    "eval_task = EvalTask(\n",
    "        dataset=df,\n",
    "        metrics=[\"exact_match\", \"rouge\"],\n",
    "        experiment=\"multimodal-hypernym-semantics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, generation_config=generation_config):\n",
    "    payload = prompt_to_payload(prompt, generation_config)\n",
    "    output = deployed_model.predict(instances=[payload])\n",
    "    generated_text = output.predictions[0]\n",
    "    return generated_text\n",
    "\n",
    "def prompt_to_payload(prompt, generation_config):\n",
    "    return {\"inputs\": prompt, \"parameters\": generation_config}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'right'"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(df['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-9d5a4a1e-9d59-40bb-b1b9-ba93c37043cd\" href=\"#view-view-vertex-resource-9d5a4a1e-9d59-40bb-b1b9-ba93c37043cd\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-9d5a4a1e-9d59-40bb-b1b9-ba93c37043cd');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/multimodal-hypernym-semantics/runs?project=multimodal-representations');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/multimodal-hypernym-semantics/runs?project=multimodal-representations', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Associating projects/841337720906/locations/us-central1/metadataStores/default/contexts/multimodal-hypernym-semantics-test-gqa-ebfeb9dd to Experiment: multimodal-hypernym-semantics\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.metadata.experiment_resources:Associating projects/841337720906/locations/us-central1/metadataStores/default/contexts/multimodal-hypernym-semantics-test-gqa-ebfeb9dd to Experiment: multimodal-hypernym-semantics\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-e1cdf826-ce6e-4e95-99b0-2b988af8152c\" href=\"#view-view-vertex-resource-e1cdf826-ce6e-4e95-99b0-2b988af8152c\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment Run</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-e1cdf826-ce6e-4e95-99b0-2b988af8152c');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/multimodal-hypernym-semantics/runs/multimodal-hypernym-semantics-test-gqa-ebfeb9dd?project=multimodal-representations');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/multimodal-hypernym-semantics/runs/multimodal-hypernym-semantics-test-gqa-ebfeb9dd?project=multimodal-representations', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating a total of 100 responses from the custom model function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vertexai.evaluation._evaluation:Generating a total of 100 responses from the custom model function.\n",
      "  0%|                                                                                                                                                                                                                                                                                                                                | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132997\n",
      "198489\n",
      "172737\n",
      "234698\n",
      "127630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|████████████▍                                                                                                                                                                                                                                                                                                           | 4/100 [00:01<00:28,  3.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157167\n",
      "135150\n",
      "298287\n",
      "121740\n",
      "87936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████████████████████▊                                                                                                                                                                                                                                                                                                  | 7/100 [00:02<00:22,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214713\n",
      "214741\n",
      "206115\n",
      "215814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████████████████████████████                                                                                                                                                                                                                                                                                        | 10/100 [00:02<00:17,  5.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|████████████████████████████████████████▍                                                                                                                                                                                                                                                                              | 13/100 [00:02<00:12,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168435\n",
      "105646\n",
      "197822\n",
      "194274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████████████████████████████████████████████▋                                                                                                                                                                                                                                                                        | 15/100 [00:03<00:12,  7.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112225\n",
      "271951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|███████████████████████████████████████████████████████████                                                                                                                                                                                                                                                            | 19/100 [00:03<00:09,  8.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134934\n",
      "127943\n",
      "138028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                        | 20/100 [00:03<00:09,  8.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87047\n",
      "87047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                            | 24/100 [00:04<00:08,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219696\n",
      "263033\n",
      "171540\n",
      "207462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|███████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                                                | 28/100 [00:04<00:07,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210578\n",
      "210595\n",
      "128465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|█████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                         | 30/100 [00:04<00:06, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148715\n",
      "594162\n",
      "148707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                             | 34/100 [00:05<00:08,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195023\n",
      "324152\n",
      "177383\n",
      "230104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                       | 36/100 [00:05<00:07,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                       | 41/100 [00:05<00:05, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151999\n",
      "136009\n",
      "247488\n",
      "202183\n",
      "183056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                 | 43/100 [00:06<00:06,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178430\n",
      "135115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                              | 44/100 [00:06<00:07,  7.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157840124214\n",
      "\n",
      "270575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                 | 48/100 [00:06<00:05, 10.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45867\n",
      "152245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                           | 50/100 [00:06<00:05,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126901\n",
      "281933\n",
      "160479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                     | 52/100 [00:07<00:05,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93343\n",
      "174218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                  | 58/100 [00:07<00:04,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230163\n",
      "174864\n",
      "179054\n",
      "255483\n",
      "211963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                         | 61/100 [00:08<00:05,  6.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152247\n",
      "157304\n",
      "157309\n",
      "157293\n",
      "149352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                      | 67/100 [00:08<00:04,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106604\n",
      "149889\n",
      "98502\n",
      "307250\n",
      "115694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                             | 70/100 [00:09<00:04,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88720\n",
      "190979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                       | 72/100 [00:09<00:03,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179059\n",
      "179033\n",
      "215814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                             | 75/100 [00:09<00:02,  8.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179035\n",
      "125426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                 | 79/100 [00:10<00:02,  7.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183825\n",
      "221300\n",
      "139533\n",
      "159568\n",
      "117054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                              | 85/100 [00:10<00:01,  8.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269561\n",
      "244655\n",
      "120226\n",
      "126176\n",
      "171018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                               | 90/100 [00:11<00:01,  7.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190401\n",
      "147129\n",
      "147127\n",
      "155359\n",
      "190835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                         | 92/100 [00:12<00:01,  4.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100999\n",
      "154638\n",
      "162182\n",
      "24990\n",
      "25006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 100 responses are successfully generated from the custom model function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:vertexai.evaluation._evaluation:All 100 responses are successfully generated from the custom model function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multithreaded Batch Inference took: 12.619170966965612 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vertexai.evaluation._evaluation:Multithreaded Batch Inference took: 12.619170966965612 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing metrics with a total of 200 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vertexai.evaluation._evaluation:Computing metrics with a total of 200 Vertex Gen AI Evaluation Service API requests.\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [13:16<00:00,  3.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 200 metric requests are successfully computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:vertexai.evaluation._evaluation:All 200 metric requests are successfully computed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Took:796.4613361860393 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:vertexai.evaluation._evaluation:Evaluation Took:796.4613361860393 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        \n",
       "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
       "    <style>\n",
       "      .view-vertex-resource,\n",
       "      .view-vertex-resource:hover,\n",
       "      .view-vertex-resource:visited {\n",
       "        position: relative;\n",
       "        display: inline-flex;\n",
       "        flex-direction: row;\n",
       "        height: 32px;\n",
       "        padding: 0 12px;\n",
       "          margin: 4px 18px;\n",
       "        gap: 4px;\n",
       "        border-radius: 4px;\n",
       "\n",
       "        align-items: center;\n",
       "        justify-content: center;\n",
       "        background-color: rgb(255, 255, 255);\n",
       "        color: rgb(51, 103, 214);\n",
       "\n",
       "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
       "        font-size: 13px;\n",
       "        font-weight: 500;\n",
       "        text-transform: uppercase;\n",
       "        text-decoration: none !important;\n",
       "\n",
       "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
       "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active {\n",
       "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
       "      }\n",
       "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
       "        position: absolute;\n",
       "        top: 0;\n",
       "        bottom: 0;\n",
       "        left: 0;\n",
       "        right: 0;\n",
       "        border-radius: 4px;\n",
       "        pointer-events: none;\n",
       "\n",
       "        content: '';\n",
       "        background-color: rgb(51, 103, 214);\n",
       "        opacity: 0.12;\n",
       "      }\n",
       "      .view-vertex-icon {\n",
       "        font-size: 18px;\n",
       "      }\n",
       "    </style>\n",
       "  \n",
       "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-880b0104-e458-41e8-9df1-6d780edf1e46\" href=\"#view-view-vertex-resource-880b0104-e458-41e8-9df1-6d780edf1e46\">\n",
       "          <span class=\"material-icons view-vertex-icon\">science</span>\n",
       "          <span>View Experiment</span>\n",
       "        </a>\n",
       "        \n",
       "        <script>\n",
       "          (function () {\n",
       "            const link = document.getElementById('view-vertex-resource-880b0104-e458-41e8-9df1-6d780edf1e46');\n",
       "            link.addEventListener('click', (e) => {\n",
       "              if (window.google?.colab?.openUrl) {\n",
       "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/multimodal-hypernym-semantics/runs?project=multimodal-representations');\n",
       "              } else {\n",
       "                window.open('https://console.cloud.google.com/vertex-ai/experiments/locations/us-central1/experiments/multimodal-hypernym-semantics/runs?project=multimodal-representations', '_blank');\n",
       "              }\n",
       "              e.stopPropagation();\n",
       "              e.preventDefault();\n",
       "            });\n",
       "          })();\n",
       "        </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "# 3. run eval task\n",
    "# Note: If the last iteration takes > 1 minute you might need to retry the evaluation\n",
    "exp_results = eval_task.evaluate(\n",
    "        model=generate, experiment_run_name=f\"test-gqa-{str(uuid.uuid4())[:8]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'row_count': 100, 'exact_match/mean': np.float64(0.28), 'exact_match/std': np.float64(0.4512608598542129), 'rouge/mean': np.float64(0.290666667), 'rouge/std': np.float64(0.451213105135993)}\n",
      "0.28\n",
      "test: 0.28\n"
     ]
    }
   ],
   "source": [
    "print(exp_results.summary_metrics)\n",
    "print(f\"{exp_results.summary_metrics['exact_match/mean']}\")\n",
    "results[\"test\"] = exp_results.summary_metrics[\"exact_match/mean\"]\n",
    "\n",
    "for prompt_name, score in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{prompt_name}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is the curtain on the right side or on the lef...</td>\n",
       "      <td>right</td>\n",
       "      <td>right</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is located on top of the coffee table?</td>\n",
       "      <td>book</td>\n",
       "      <td>goldgold bara tier tier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is the surfer to the left or to the right of t...</td>\n",
       "      <td>right</td>\n",
       "      <td>right orson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which kind of fast food is the bacon on?</td>\n",
       "      <td>pizza</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is the boat on the right of the picture?</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Is there any bottle on the table?</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Do you see white breads?</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Which color is the skateboard in the bottom?</td>\n",
       "      <td>brown</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Is the orange cat sitting on a desk?</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>What is the name of the animal on top of the t...</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question reference  \\\n",
       "0   Is the curtain on the right side or on the lef...     right   \n",
       "1         What is located on top of the coffee table?      book   \n",
       "2   Is the surfer to the left or to the right of t...     right   \n",
       "3            Which kind of fast food is the bacon on?     pizza   \n",
       "4            Is the boat on the right of the picture?        no   \n",
       "..                                                ...       ...   \n",
       "95                  Is there any bottle on the table?       yes   \n",
       "96                           Do you see white breads?       yes   \n",
       "97       Which color is the skateboard in the bottom?     brown   \n",
       "98               Is the orange cat sitting on a desk?        no   \n",
       "99  What is the name of the animal on top of the t...       cat   \n",
       "\n",
       "                   response  \n",
       "0                     right  \n",
       "1   goldgold bara tier tier  \n",
       "2               right orson  \n",
       "3                      food  \n",
       "4                        no  \n",
       "..                      ...  \n",
       "95                       no  \n",
       "96                       no  \n",
       "97                     blue  \n",
       "98                       no  \n",
       "99                      cat  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results.metrics_table[['question','reference','response']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch inference\n",
    "\n",
    "Below is example code from the GCP documentation found at (https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions)[https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions]\n",
    "\n",
    "Also check (https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/batch_eval_llm.ipynb)[https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/batch_eval_llm.ipynb]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_prediction_job_dedicated_resources_sample(\n",
    "    model,\n",
    "    job_display_name: str,\n",
    "    gcs_source,\n",
    "    gcs_destination: str,\n",
    "    machine_type=\"g2-standard-4\", #$0.8129 USD / hour\n",
    "    accelerator_type=\"NVIDIA_L4\", #$0.644046 USD / hour\n",
    "    accelerator_count=1,\n",
    "    instances_format: str = \"jsonl\",\n",
    "    starting_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    sync: bool = True,\n",
    "):\n",
    "\n",
    "\n",
    "    batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        gcs_source=gcs_source,\n",
    "        gcs_destination_prefix=gcs_destination,\n",
    "        instances_format=instances_format,\n",
    "        starting_replica_count=starting_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        \n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    batch_prediction_job.wait()\n",
    "\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.resource_name)\n",
    "    print(batch_prediction_job.state)\n",
    "    return batch_prediction_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        gcs_source=gcs_source,\n",
    "        gcs_destination_prefix=gcs_destination,\n",
    "        instances_format=instances_format,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        accelerator_type=accelerator_type,\n",
    "        starting_replica_count=starting_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        sync=sync,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource clean-up (DEFINITELY DO THIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can already release the resources that you've created as follows, to avoid unnecessary costs:\n",
    "\n",
    "* `deployed_model.undeploy_all` to undeploy the model from all the endpoints.\n",
    "* `deployed_model.delete` to delete the endpoint/s where the model was deployed gracefully, after the `undeploy_all` method.\n",
    "* `model.delete` to delete the model from the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployed_model.undeploy_all()\n",
    "deployed_model.delete()\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also remove those from the Google Cloud Console following the steps:\n",
    "\n",
    "* Go to Vertex AI in Google Cloud\n",
    "* Go to Deploy and use -> Online prediction\n",
    "* Click on the endpoint and then on the deployed model/s to \"Undeploy model from endpoint\"\n",
    "* Then go back to the endpoint list and remove the endpoint\n",
    "* Finally, go to Deploy and use -> Model Registry, and remove the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable APIs\n",
    "\n",
    "!gcloud services disable aiplatform.googleapis.com\n",
    "!gcloud services disable compute.googleapis.com\n",
    "!gcloud services disable container.googleapis.com\n",
    "!gcloud services disable containerregistry.googleapis.com\n",
    "!gcloud services disable containerfilesystem.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLEASE ALSO MANUALLY ENSURE ALL APIS ARE DISABLED ON GCP AFTER THIS IS DONE!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
