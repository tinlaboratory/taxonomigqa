{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch inference with Gemma/PaliGemma with HF + GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gemma is a family of lightweight, state-of-the-art open models built from the same research and technology used to create the Gemini models, developed by Google DeepMind and other teams across Google. Text Generation Inference (TGI) is a toolkit developed by Hugging Face for deploying and serving LLMs, with high performance text generation. And, Google Vertex AI is a Machine Learning (ML) platform that lets you train and deploy ML models and AI applications, and customize large language models (LLMs) for use in your AI-powered applications. This example showcases how to deploy any supported text-generation model, in this case [`google/gemma-7b-it`](https://huggingface.co/google/gemma-7b-it), from the Hugging Face Hub on Vertex AI using the TGI DLC available in Google Cloud Platform (GCP)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![`google/gemma-7b-it` in the Hugging Face Hub](./assets/model-in-hf-hub.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup / Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you need to install `gcloud` in your local machine, which is the command-line tool for Google Cloud, following the instructions at [Cloud SDK Documentation - Install the gcloud CLI](https://cloud.google.com/sdk/docs/install)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, you also need to install the `google-cloud-aiplatform` Python SDK, required to programmatically create the Vertex AI model, register it, acreate the endpoint, and deploy it on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (1.72.0)\n",
      "Requirement already satisfied: google-auth in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (2.36.0)\n",
      "Requirement already satisfied: google-cloud-pipeline-components in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (1.0.45)\n",
      "Requirement already satisfied: packaging in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (24.2)\n",
      "Requirement already satisfied: tensorflow in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.10.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (1.25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (3.18.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (1.6.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (2.10.2)\n",
      "Requirement already satisfied: docstring-parser<1 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-auth) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-auth) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-auth) (4.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-pipeline-components) (1.65.0)\n",
      "Requirement already satisfied: grpcio-status<=1.47.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-pipeline-components) (1.47.0)\n",
      "Requirement already satisfied: kfp<2.0.0,>=1.8.9 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-pipeline-components) (1.8.22)\n",
      "Requirement already satisfied: google-cloud-notebooks>=0.4.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-pipeline-components) (1.4.4)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (75.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (1.67.1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.6.0)\n",
      "Requirement already satisfied: rich in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (13.9.3)\n",
      "Requirement already satisfied: namex in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (6.0.2)\n",
      "Requirement already satisfied: kubernetes<26,>=8.0.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (25.3.0)\n",
      "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.12.11)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.10.1)\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (2.2.1)\n",
      "Requirement already satisfied: kfp-server-api<2.0.0,>=1.1.2 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.8.5)\n",
      "Requirement already satisfied: jsonschema<5,>=3.0.1 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (4.23.0)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.9.0)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (8.1.7)\n",
      "Requirement already satisfied: Deprecated<2,>=1.2.7 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.2.14)\n",
      "Requirement already satisfied: strip-hints<1,>=0.1.8 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.1.10)\n",
      "Requirement already satisfied: kfp-pipeline-spec<0.2.0,>=0.1.16 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.1.16)\n",
      "Requirement already satisfied: fire<1,>=0.3.1 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.7.0)\n",
      "Requirement already satisfied: uritemplate<4,>=3.0.1 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (3.0.1)\n",
      "Requirement already satisfied: urllib3<2 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.26.20)\n",
      "Collecting pydantic<3 (from google-cloud-aiplatform)\n",
      "  Using cached pydantic-1.10.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (152 kB)\n",
      "Requirement already satisfied: typer<1.0,>=0.3.2 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.12.5)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.2)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.2.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from jsonschema<5,>=3.0.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (24.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from jsonschema<5,>=3.0.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from jsonschema<5,>=3.0.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from jsonschema<5,>=3.0.1->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (0.20.1)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kubernetes<26,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from kubernetes<26,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (2.0.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from typer<1.0,>=0.3.2->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (1.5.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from httplib2<1dev,>=0.15.0->google-api-python-client<2,>=1.7.8->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (3.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/dali/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages (from requests-oauthlib->kubernetes<26,>=8.0.0->kfp<2.0.0,>=1.8.9->google-cloud-pipeline-components) (3.2.2)\n",
      "Using cached pydantic-1.10.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "Installing collected packages: pydantic\n",
      "  Attempting uninstall: pydantic\n",
      "    Found existing installation: pydantic 2.10.2\n",
      "    Uninstalling pydantic-2.10.2:\n",
      "      Successfully uninstalled pydantic-2.10.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradio 4.21.0 requires pydantic>=2.0, but you have pydantic 1.10.19 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pydantic-1.10.19\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install  google-cloud-aiplatform google-auth google-cloud-pipeline-components packaging tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, to ease the usage of the commands within this tutorial, you need to set the following environment variables for GCP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=multimodal-representations\n",
      "env: LOCATION=us-central1\n",
      "env: CONTAINER_URI=us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu124.2-3.ubuntu2204.py311\n"
     ]
    }
   ],
   "source": [
    "%env PROJECT_ID=multimodal-representations\n",
    "%env LOCATION=us-central1\n",
    "#%env CONTAINER_URI=us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-2.ubuntu2204.py310:latest\n",
    "#%env CONTAINER_URI=us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-llava-serve\n",
    "%env CONTAINER_URI=us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu124.2-3.ubuntu2204.py311\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you need to login into your GCP account and set the project ID to the one you want to use to register and deploy the models on Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your browser has been opened to visit:\n",
      "\n",
      "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=32555940559.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8085%2F&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fappengine.admin+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcompute+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Faccounts.reauth&state=FrkP4LpKlMgwzrw1rhmHyXBB26Ke1B&access_type=offline&code_challenge=mSstYiivXM-2vjYq7f7e3ATcchSQkUjUwsFCB9Arux0&code_challenge_method=S256\n",
      "\n",
      "\n",
      "You are now logged in as [daliumuwork@gmail.com].\n",
      "Your current project is [multimodal-representations].  You can change this setting by running:\n",
      "  $ gcloud config set project PROJECT_ID\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n",
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud auth login\n",
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you are logged in, you need to enable the necessary service APIs in GCP, such as the Vertex AI API, the Compute Engine API, and Google Container Registry related APIs.\n",
    "\n",
    "**Warning:** Make sure, manually, that these are disabled after running exps (even though we will explicitly write code to disable them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com\n",
    "!gcloud services enable compute.googleapis.com\n",
    "!gcloud services enable container.googleapis.com\n",
    "!gcloud services enable containerregistry.googleapis.com\n",
    "!gcloud services enable containerfilesystem.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'vertex-ai-samples' already exists and is not an empty directory.\n",
      "\u001b[33mWARNING: typer 0.12.5 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Enabling Vertex AI API and Compute Engine API.\n",
      "Operation \"operations/acat.p2-841337720906-a8349369-bf69-42cf-9318-fa38ec919a93\" finished successfully.\n",
      "Using this GCS Bucket: gs://multimodal-representations-eval-data-central1/\n",
      "Initializing Vertex AI API.\n",
      "Using this default Service Account: 841337720906-compute@developer.gserviceaccount.com\n",
      "No changes made to gs://multimodal-representations-eval-data-central1/\n",
      "Updated property [core/project].\n",
      "Copying PaliGemma model artifacts from gs://vertex-model-garden-paligemma-us/paligemma to  gs://multimodal-representations-eval-data-central1/paligemma\n",
      "Copying gs://vertex-model-garden-paligemma-us/paligemma/mix_224.npz [Content-Type=application/octet-stream]...\n",
      "\\ [1/1 files][ 10.9 GiB/ 10.9 GiB] 100% Done  24.8 MiB/s ETA 00:00:00           \n",
      "Operation completed over 1 objects/10.9 GiB.                                     \n",
      "Copying gs://vertex-model-garden-paligemma-us/paligemma/pt_224.npz [Content-Type=application/octet-stream]...\n",
      "- [1/1 files][ 10.9 GiB/ 10.9 GiB] 100% Done     0.0 B/s                        \n",
      "Operation completed over 1 objects/10.9 GiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# @title Setup Google Cloud project\n",
    "# @markdown 1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "# @markdown 2. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead.\n",
    "\n",
    "BUCKET_URI = \"gs://multimodal-representations-eval-data-central1/\"  # @param {type:\"string\"}\n",
    "\n",
    "# @markdown 3. **[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment.\n",
    "\n",
    "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
    "\n",
    "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git\n",
    "\n",
    "# Import the necessary packages\n",
    "! pip install -q gradio==4.21.0\n",
    "import datetime\n",
    "import enum\n",
    "import importlib\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "from typing import Sequence, Tuple\n",
    "\n",
    "import gradio as gr\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from google.cloud import aiplatform\n",
    "from PIL import Image\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")\n",
    "\n",
    "models, endpoints = {}, {}\n",
    "\n",
    "# Get the default cloud project id.\n",
    "PROJECT_ID = os.environ[\"PROJECT_ID\"]\n",
    "\n",
    "# Get the default region for launching jobs.\n",
    "if not REGION:\n",
    "    REGION = os.environ[\"LOCATION\"]\n",
    "\n",
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
    "\n",
    "# Cloud Storage bucket for storing the experiment artifacts.\n",
    "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
    "# prefer using your own GCS bucket, change the value yourself below.\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "\n",
    "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
    "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
    "else:\n",
    "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
    "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
    "    bucket_region = shell_output[0].strip().lower()\n",
    "    if bucket_region != REGION:\n",
    "        raise ValueError(\n",
    "            \"Bucket region %s is different from notebook region %s\"\n",
    "            % (bucket_region, REGION)\n",
    "        )\n",
    "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"paligemma\")\n",
    "\n",
    "\n",
    "# Initialize Vertex AI API.\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)\n",
    "\n",
    "# Gets the default SERVICE_ACCOUNT.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
    "\n",
    "\n",
    "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
    "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\"\n",
    "\n",
    "# @markdown ### Access PaliGemma models on Vertex AI for GPU based serving\n",
    "# @markdown Accept the model agreement to access the models:\n",
    "# @markdown 1. Open the [PaliGemma model card](https://console.cloud.google.com/vertex-ai/publishers/google/model-garden/363) from [Vertex AI Model Garden](https://cloud.google.com/model-garden).\n",
    "# @markdown 1. Review and accept the agreement in the pop-up window on the model card page. If you have previously accepted the model agreement, there will not be a pop-up window on the model card page and this step is not needed.\n",
    "# @markdown 1. After accepting the agreement of PaliGemma, a `gs://` URI containing PaliGemma pretrained models will be shared.\n",
    "# @markdown 1. Paste the link in the `VERTEX_AI_MODEL_GARDEN_PALIGEMMA` field below.\n",
    "# @markdown 1. The PaliGemma models will be copied into `BUCKET_URI`.\n",
    "# @markdown The file transfer can take anywhere from 15 minutes to 30 minutes.\n",
    "VERTEX_AI_MODEL_GARDEN_PALIGEMMA = \"gs://vertex-model-garden-paligemma-us/paligemma\"  # @param {type:\"string\", isTemplate:true}\n",
    "assert (\n",
    "    VERTEX_AI_MODEL_GARDEN_PALIGEMMA\n",
    "), \"Click the agreement of PaliGemma in Vertex AI Model Garden, and get the GCS path of PaliGemma model artifacts.\"\n",
    "print(\n",
    "    \"Copying PaliGemma model artifacts from\",\n",
    "    VERTEX_AI_MODEL_GARDEN_PALIGEMMA,\n",
    "    \"to \",\n",
    "    MODEL_BUCKET,\n",
    ")\n",
    "\n",
    "#! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_PALIGEMMA/pt_224.npz $MODEL_BUCKET\n",
    "! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_PALIGEMMA/mix_224.npz os.path.join(BUCKET_URI, \"mix_224.npz\")\n",
    "! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_PALIGEMMA/pt_224.npz os.path.join(BUCKET_URI, \"pt_224.npz\")\n",
    "\n",
    "\n",
    "model_path_prefix = MODEL_BUCKET\n",
    "pretrained_filename_lookup = {\n",
    "    \"paligemma-224-float32\": \"pt_224.npz\",\n",
    "    \"paligemma-224-float16\": \"pt_224.f16.npz\",\n",
    "    \"paligemma-mix-224-float32\": \"mix_224.npz\",\n",
    "    \"paligemma-mix-224-float16\": \"mix_224.f16.npz\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(\n",
    "    model_name: str,\n",
    "    checkpoint_path: str,\n",
    "    machine_type: str = \"g2-standard-32\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    resolution: int = 224,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Create a Vertex AI Endpoint and deploy the specified model to the endpoint.\"\"\"\n",
    "    model_name_with_time = common_util.get_job_name_with_datetime(model_name)\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=f\"{model_name_with_time}-endpoint\"\n",
    "    )\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name_with_time,\n",
    "        serving_container_image_uri=SERVE_DOCKER_URI,\n",
    "        serving_container_ports=[8080],\n",
    "        serving_container_predict_route=\"/predict\",\n",
    "        serving_container_health_route=\"/health\",\n",
    "        serving_container_environment_variables={\n",
    "            \"CKPT_PATH\": checkpoint_path,\n",
    "            \"RESOLUTION\": resolution,\n",
    "            \"MODEL_ID\": \"google/\" + model_name,\n",
    "        },\n",
    "    )\n",
    "    print(\n",
    "        f\"Deploying {model_name_with_time} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
    "    )\n",
    "    deployed_model = model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=SERVICE_ACCOUNT,\n",
    "        enable_access_logging=True,\n",
    "        min_replica_count=1,\n",
    "        sync=True,\n",
    "    )\n",
    "    return deployed_model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "endpoints = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying custom PaliGemma model.\n"
     ]
    }
   ],
   "source": [
    "# @title Deploy\n",
    "\n",
    "# @markdown This section uploads the prebuilt PaliGemma model to Model Registry and deploys it to a Vertex AI Endpoint. It takes approximately 15 minutes to finish.\n",
    "\n",
    "# @markdown Select the desired resolution and precision of prebuilt model to deploy, leaving the optional `custom_paligemma_model_uri` as is. Higher resolution and precision_type can result in better inference results, but may require additional GPU.\n",
    "\n",
    "# @markdown You can also serve a finetuned PaliGemma model by setting `resolution` and `precision_type` to the resolution and precision type of the original base model and then setting `custom_paligemma_model_uri` to the GCS URI containing the model.\n",
    "\n",
    "# @markdown **Note**: You cannot use accelerator type `NVIDIA_TESLA_V100` to serve prebuilt or finetuned PaliGemma models with resolution `896` and precision_type `float32`.\n",
    "\n",
    "model_variant = \"mix\"  # @param [\"mix\", \"pt\"]\n",
    "resolution = 224  # @param [224, 448, 896]\n",
    "precision_type = \"float32\"  # @param [\"float32\", \"float16\", \"bfloat16\"]\n",
    "custom_paligemma_model_uri = \"gs://\"#vertex-model-garden-paligemma-us/paligemma/mix_224.npz\"  # @param {type: \"string\"}\n",
    "\n",
    "if model_variant == \"mix\":\n",
    "    model_name_prefix = \"paligemma-mix\"\n",
    "else:\n",
    "    model_name_prefix = \"paligemma\"\n",
    "\n",
    "if custom_paligemma_model_uri == \"gs://\" or not custom_paligemma_model_uri:\n",
    "    print(\"Deploying prebuilt PaliGemma model.\")\n",
    "    model_name = f\"{model_name_prefix}-{resolution}-{precision_type}\"\n",
    "    checkpoint_filename = pretrained_filename_lookup[model_name]\n",
    "    checkpoint_path = os.path.join(model_path_prefix, checkpoint_filename)\n",
    "else:\n",
    "    print(\"Deploying custom PaliGemma model.\")\n",
    "    model_name = f\"{model_name_prefix}-{resolution}-{precision_type}\"\n",
    "    checkpoint_path = custom_paligemma_model_uri\n",
    "\n",
    "# The pre-built serving docker image.\n",
    "SERVE_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/jax-paligemma-serve-gpu:20240807_0916_RC00\"\n",
    "\n",
    "# @markdown If you want to use other accelerator types not listed below, then check other Vertex AI prediction supported accelerators and regions at https://cloud.google.com/vertex-ai/docs/predictions/configure-compute. You may need to manually set the `machine_type`, `accelerator_type`, and `accelerator_count` in the code by clicking `Show code` first.\n",
    "# @markdown Select the accelerator type to use to deploy the model:\n",
    "accelerator_type = \"NVIDIA_L4\"  # @param [\"NVIDIA_L4\", \"NVIDIA_TESLA_V100\"]\n",
    "machine_type = \"g2-standard-32\"\n",
    "accelerator_count = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy(model_file):\n",
    "    checkpoint_path = \"gs://vertex-model-garden-paligemma-us/paligemma/{model_file}\"\n",
    "    return deploy_model(\n",
    "        model_name=model_name,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        resolution=resolution,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/841337720906/locations/us-central1/endpoints/2625749215850004480/operations/2686509774589132800\n",
      "Endpoint created. Resource name: projects/841337720906/locations/us-central1/endpoints/2625749215850004480\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/841337720906/locations/us-central1/endpoints/2625749215850004480')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/841337720906/locations/us-central1/models/1951489103279161344/operations/544837045541928960\n",
      "Model created. Resource name: projects/841337720906/locations/us-central1/models/1951489103279161344@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/841337720906/locations/us-central1/models/1951489103279161344@1')\n",
      "Deploying paligemma-mix-224-float32-20241128-150212 on g2-standard-32 with 1 NVIDIA_L4 GPU(s).\n",
      "Deploying model to Endpoint : projects/841337720906/locations/us-central1/endpoints/2625749215850004480\n",
      "Deploy Endpoint model backing LRO: projects/841337720906/locations/us-central1/endpoints/2625749215850004480/operations/5663389128281030656\n"
     ]
    },
    {
     "ename": "FailedPrecondition",
     "evalue": "400 Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=841337720906&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%222625749215850004480%22%0Aresource.labels.location%3D%22us-central1%22.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPrecondition\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt_224.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m model_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmix_224.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m models[model_file], endpoint[model_file] \u001b[38;5;241m=\u001b[39m \u001b[43mdeploy_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_BUCKET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mdeploy_model\u001b[0;34m(model_name, checkpoint_path, machine_type, accelerator_type, accelerator_count, resolution)\u001b[0m\n\u001b[1;32m     14\u001b[0m model \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mupload(\n\u001b[1;32m     15\u001b[0m     display_name\u001b[38;5;241m=\u001b[39mmodel_name_with_time,\n\u001b[1;32m     16\u001b[0m     serving_container_image_uri\u001b[38;5;241m=\u001b[39mSERVE_DOCKER_URI,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m     },\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeploying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_with_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmachine_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccelerator_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccelerator_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m GPU(s).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m deployed_model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1800\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSERVICE_ACCOUNT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_access_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m deployed_model, endpoint\n",
      "File \u001b[0;32m~/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages/google/cloud/aiplatform/models.py:5261\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, tpu_topology, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, enable_access_logging, disable_container_logging, private_service_connect_config, deployment_resource_pool, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, spot)\u001b[0m\n\u001b[1;32m   5250\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   5251\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraffic splitting is not yet supported for PSA based PrivateEndpoint. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5252\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry calling deploy() without providing `traffic_split`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5253\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA maximum of one model can be deployed to each private Endpoint.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5254\u001b[0m         )\n\u001b[1;32m   5256\u001b[0m explanation_spec \u001b[38;5;241m=\u001b[39m _explanation_utils\u001b[38;5;241m.\u001b[39mcreate_and_validate_explanation_spec(\n\u001b[1;32m   5257\u001b[0m     explanation_metadata\u001b[38;5;241m=\u001b[39mexplanation_metadata,\n\u001b[1;32m   5258\u001b[0m     explanation_parameters\u001b[38;5;241m=\u001b[39mexplanation_parameters,\n\u001b[1;32m   5259\u001b[0m )\n\u001b[0;32m-> 5261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraffic_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraffic_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5265\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraffic_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraffic_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5269\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5270\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_topology\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_topology\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreservation_affinity_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreservation_affinity_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreservation_affinity_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplanation_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5277\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5278\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencryption_spec_key_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption_spec_key_name\u001b[49m\n\u001b[1;32m   5279\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencryption_spec_key_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5281\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5286\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_access_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_access_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_container_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_container_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprivate_service_connect_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprivate_service_connect_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeployment_resource_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployment_resource_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5290\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages/google/cloud/aiplatform/models.py:5476\u001b[0m, in \u001b[0;36mModel._deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, tpu_topology, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, service_account, explanation_spec, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, spot, enable_access_logging, disable_container_logging, private_service_connect_config, deployment_resource_pool)\u001b[0m\n\u001b[1;32m   5464\u001b[0m         endpoint \u001b[38;5;241m=\u001b[39m PrivateEndpoint\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m   5465\u001b[0m             display_name\u001b[38;5;241m=\u001b[39mdisplay_name,\n\u001b[1;32m   5466\u001b[0m             network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5471\u001b[0m             private_service_connect_config\u001b[38;5;241m=\u001b[39mprivate_service_connect_config,\n\u001b[1;32m   5472\u001b[0m         )\n\u001b[1;32m   5474\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_start_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeploying model to\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, endpoint)\n\u001b[0;32m-> 5476\u001b[0m \u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deploy_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5479\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5480\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraffic_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5482\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5483\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraffic_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraffic_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraffic_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraffic_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5488\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5489\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_topology\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_topology\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreservation_affinity_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreservation_affinity_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreservation_affinity_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreservation_affinity_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplanation_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5501\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_access_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_access_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5502\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_container_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_container_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5503\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeployment_resource_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployment_resource_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5504\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5506\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeployed\u001b[39m\u001b[38;5;124m\"\u001b[39m, endpoint)\n\u001b[1;32m   5508\u001b[0m endpoint\u001b[38;5;241m.\u001b[39m_sync_gca_resource()\n",
      "File \u001b[0;32m~/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages/google/cloud/aiplatform/models.py:1957\u001b[0m, in \u001b[0;36mEndpoint._deploy_call\u001b[0;34m(cls, api_client, endpoint_resource_name, model, endpoint_resource_traffic_split, network, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, tpu_topology, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, service_account, explanation_spec, metadata, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, spot, enable_access_logging, disable_container_logging, deployment_resource_pool)\u001b[0m\n\u001b[1;32m   1945\u001b[0m operation_future \u001b[38;5;241m=\u001b[39m api_client\u001b[38;5;241m.\u001b[39mdeploy_model(\n\u001b[1;32m   1946\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint_resource_name,\n\u001b[1;32m   1947\u001b[0m     deployed_model\u001b[38;5;241m=\u001b[39mdeployed_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1950\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mdeploy_request_timeout,\n\u001b[1;32m   1951\u001b[0m )\n\u001b[1;32m   1953\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_started_against_resource_with_lro(\n\u001b[1;32m   1954\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeploy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m, operation_future\n\u001b[1;32m   1955\u001b[0m )\n\u001b[0;32m-> 1957\u001b[0m \u001b[43moperation_future\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/postdoc/multimodal-semantic-signals/multimodal-representations/notebook-env/lib/python3.12/site-packages/google/api_core/future/polling.py:137\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking_poll(timeout\u001b[38;5;241m=\u001b[39mtimeout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "\u001b[0;31mFailedPrecondition\u001b[0m: 400 Model server exited unexpectedly. Model server logs can be found at https://console.cloud.google.com/logs/viewer?project=841337720906&resource=aiplatform.googleapis.com%2FEndpoint&advancedFilter=resource.type%3D%22aiplatform.googleapis.com%2FEndpoint%22%0Aresource.labels.endpoint_id%3D%222625749215850004480%22%0Aresource.labels.location%3D%22us-central1%22."
     ]
    }
   ],
   "source": [
    "\n",
    "model_file = \"pt_224.npz\"\n",
    "model_file = \"mix_224.npz\"\n",
    "models[model_file], endpoint[model_file] = deploy_model(\n",
    "        model_name=model_name,\n",
    "        checkpoint_path=MODEL_BUCKET,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        resolution=resolution,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = aiplatform.Endpoint.list()\n",
    "endpoints[model_file] = eps[0]\n",
    "endpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register model on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once everything is set up, you can already initialize the Vertex AI session via the `google-cloud-aiplatform` Python SDK as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation task\n",
    "\n",
    "Based on [https://huggingface.co/docs/google-cloud/main/examples/vertex-ai-notebooks-evaluate-llms-with-vertex-ai](https://huggingface.co/docs/google-cloud/main/examples/vertex-ai-notebooks-evaluate-llms-with-vertex-ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"fgqa_hs\", split='test[:1000]')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must convert to a pandas dataset in order to use the Vertex Evaluation API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "df[\"img\"] = df[\"image\"].apply(lambda x: Image.open(x['path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df['prompt'] = df.apply(lambda row: f\"![]({row['img']}) answer en {row['question']}\\n\", axis=1)\n",
    "df['prompt'] = df.apply(lambda row: (row['img'], row['question']), axis=1)\n",
    "\n",
    "df['prompt'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reference'] = df['answer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all columns that we do not need for the prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paligemma(prompt, model='pt_224.npz'):\n",
    "    answers = common_util.vqa_predict(endpoints[model],[prompt[1]] , prompt[0])\n",
    "    return answers[0].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "image_url = \"https://images.pexels.com/photos/4012966/pexels-photo-4012966.jpeg\"  # @param {type:\"string\"}\n",
    "\n",
    "image = common_util.download_image(image_url)\n",
    "print(image)\n",
    "display(image)\n",
    "\n",
    "# @markdown You may leave question prompts empty and they will be ignored.\n",
    "question_prompt_1 = \"Which of laptop, book, pencil, clock, flower are in the image?\"  # @param {type: \"string\"}\n",
    "question_prompt_2 = \"Do the book and the cup have the same color?\"  # @param {type: \"string\"}\n",
    "question_prompt_3 = \"Is there a person in the image?\"  # @param {type: \"string\"}\n",
    "question_prompt_4 = \"How many laptop are in the image?\"  # @param {type: \"string\"}\n",
    "\n",
    "# @markdown The question prompt can be non-English languages.\n",
    "questions_list = [\n",
    "    question_prompt_1,\n",
    "    question_prompt_2,\n",
    "    question_prompt_3,\n",
    "    question_prompt_4,\n",
    "]\n",
    "questions = [question for question in questions_list if question]\n",
    "print(endpoints)\n",
    "answers = common_util.vqa_predict(endpoints[model_file], questions, image)\n",
    "\n",
    "for question, answer in zip(questions, answers):\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "# @markdown Click \"Show Code\" to see more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_paligemma(df['prompt'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.evaluation import EvalTask\n",
    "from vertexai.generative_models import (Part)\n",
    "# 2. create eval task\n",
    "eval_task = EvalTask(\n",
    "        dataset=df,\n",
    "        metrics=[\"exact_match\"],\n",
    "        experiment=\"multimodal-hypernym-semantics\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# 3. run eval task\n",
    "# Note: If the last iteration takes > 1 minute you might need to retry the evaluation\n",
    "exp_results = eval_task.evaluate(\n",
    "        model=generate_paligemma, experiment_run_name=f\"test-gqa-{str(uuid.uuid4())[:8]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(prompts, generation_config=generation_config):\n",
    "    payloads = [prompt_to_payload(prompt, generation_config) for prompt in prompts]\n",
    "    print(payloads)\n",
    "    output = endpoint.predict(instances=payloads)\n",
    "    generated_texts = output.predictions\n",
    "    #print(output.predictions)\n",
    "    return [pred.lower() for pred in generated_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "print(exp_results.summary_metrics)\n",
    "print(f\"{exp_results.summary_metrics['exact_match/mean']}\")\n",
    "results[\"test\"] = exp_results.summary_metrics[\"exact_match/mean\"]\n",
    "\n",
    "for prompt_name, score in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{prompt_name}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_results.metrics_table[['question', 'response', 'reference', 'argument', 'substitution', 'exact_match/score']]\n",
    "#exp_results.metrics_table[['question', 'response', 'reference', 'argument', 'substitution', 'exact_match/score', 'rouge/score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.loc[:,'exact_match/score'] = (result_df['reference'] == result_df['response']).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = exp_results.metrics_table[['question', 'response', 'reference', 'argument', 'substitution', 'exact_match/score']]\n",
    "result_df.loc[:,'exact_match/score'] = (result_df['reference'] == result_df['response']).astype(int)\n",
    "#result_df = exp_results.metrics_table[['question', 'response', 'reference', 'argument', 'substitution', 'exact_match/score', 'rouge/score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(result_df['response'] == result_df['reference'])/len(result_df['reference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_qs_df = result_df[result_df['substitution'] == \"\"]\n",
    "sub_qs_df = result_df[result_df['substitution'] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_qs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_qs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(original_qs_df['response'] == original_qs_df['reference'])/len(original_qs_df['reference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sub_qs_df['response'] == sub_qs_df['reference'])/len(sub_qs_df['reference'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('llava-100-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_base_questions = result_df[result_df['substitution'] == ''].groupby('argument').agg({\n",
    "    'exact_match/score': 'mean',\n",
    "  #  'rouge/score': 'mean'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_base_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating over all substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_substitutions = result_df[result_df['substitution'] != ''].groupby('argument').agg({\n",
    "    'exact_match/score': 'mean',\n",
    "#    'rouge/score': 'mean'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aggregated_substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "aggregated_combined = aggregated_base_questions.rename(columns={'exact_match/score': 'base/exact_match/score',\n",
    "                                                                #'rouge/score':'base/rouge/score'\n",
    "                                                               })\n",
    "\n",
    "# Merge the two dataframes on a common key (in this case, 'key')\n",
    "aggregated_combined = pd.merge(aggregated_combined, aggregated_substitutions, on='argument', how='left')\n",
    "\n",
    "# Fill empty values with 0.0\n",
    "aggregated_combined = aggregated_combined.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_results.metrics_table['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "from PIL import Image as PImage\n",
    "\n",
    "imgs = set([img['path'] for img in df['image'][:10]])\n",
    "\n",
    "for image in imgs:\n",
    "    #Image(filename=image['path'])\n",
    "    img = mpimg.imread(image)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    img = image_to_base64(PImage.open(image))\n",
    "    output = generate([img, \"What is in the image?\"])\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "image = mpimg.imread(image['path'])\n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch inference\n",
    "\n",
    "Below is example code from the GCP documentation found at (https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions)[https://cloud.google.com/vertex-ai/docs/predictions/get-batch-predictions]\n",
    "\n",
    "Also check (https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/batch_eval_llm.ipynb)[https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/generative_ai/batch_eval_llm.ipynb]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batch_prediction_job_dedicated_resources_sample(\n",
    "    model,\n",
    "    job_display_name: str,\n",
    "    gcs_source,\n",
    "    gcs_destination: str,\n",
    "    machine_type=\"g2-standard-24\", #$0.8129 USD / hour\n",
    "    accelerator_type=\"NVIDIA_L4\", #$0.644046 USD / hour\n",
    "    accelerator_count=2,\n",
    "    instances_format: str = \"jsonl\",\n",
    "    starting_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    sync: bool = True,\n",
    "):\n",
    "\n",
    "\n",
    "    batch_prediction_job = model.batch_predict(\n",
    "        job_display_name=job_display_name,\n",
    "        gcs_source=gcs_source,\n",
    "        gcs_destination_prefix=gcs_destination,\n",
    "        instances_format=instances_format,\n",
    "        starting_replica_count=starting_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        \n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    batch_prediction_job.wait()\n",
    "\n",
    "    print(batch_prediction_job.display_name)\n",
    "    print(batch_prediction_job.resource_name)\n",
    "    print(batch_prediction_job.state)\n",
    "    return batch_prediction_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_prediction_job = create_batch_prediction_job_dedicated_resources_sample(\n",
    "    model,\n",
    "        job_display_name=\"batch-llava-test-100\",\n",
    "        gcs_source=\"gs://multimodal-representations-eval-data/data.jsonl\",\n",
    "        gcs_destination=\"gs://multimodal-representations-eval-data/\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource clean-up (DEFINITELY DO THIS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can already release the resources that you've created as follows, to avoid unnecessary costs:\n",
    "\n",
    "* `deployed_model.undeploy_all` to undeploy the model from all the endpoints.\n",
    "* `deployed_model.delete` to delete the endpoint/s where the model was deployed gracefully, after the `undeploy_all` method.\n",
    "* `model.delete` to delete the model from the registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'deployed_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdeployed_model\u001b[49m\u001b[38;5;241m.\u001b[39mundeploy_all()\n\u001b[1;32m      2\u001b[0m deployed_model\u001b[38;5;241m.\u001b[39mdelete()\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mdelete()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'deployed_model' is not defined"
     ]
    }
   ],
   "source": [
    "deployed_model.undeploy_all()\n",
    "deployed_model.delete()\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also remove those from the Google Cloud Console following the steps:\n",
    "\n",
    "* Go to Vertex AI in Google Cloud\n",
    "* Go to Deploy and use -> Online prediction\n",
    "* Click on the endpoint and then on the deployed model/s to \"Undeploy model from endpoint\"\n",
    "* Then go back to the endpoint list and remove the endpoint\n",
    "* Finally, go to Deploy and use -> Model Registry, and remove the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable APIs\n",
    "\n",
    "!gcloud services disable aiplatform.googleapis.com\n",
    "!gcloud services disable compute.googleapis.com\n",
    "!gcloud services disable container.googleapis.com\n",
    "!gcloud services disable containerregistry.googleapis.com\n",
    "!gcloud services disable containerfilesystem.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLEASE ALSO MANUALLY ENSURE ALL APIS ARE DISABLED ON GCP AFTER THIS IS DONE!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an image from Google Cloud Storage\n",
    "# Load from local file\n",
    "from vertexai.generative_models import Image as V_Image\n",
    "\n",
    "gen_model = GenerativeModel(\"paligemma\")\n",
    "\n",
    "image = V_Image.load_from_file(df['image'][0]['path'])\n",
    "\n",
    "# Prepare contents\n",
    "prompt = \"Describe this image?\"\n",
    "contents = [image, prompt]\n",
    "\n",
    "response = gen_model.generate_content(contents)\n",
    "\n",
    "print(\"-------Prompt--------\")\n",
    "print_multimodal_prompt(contents)\n",
    "\n",
    "print(\"\\n-------Response--------\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import aiplatform\n",
    "endpoints = aiplatform.Endpoint.list()\n",
    "for i in endpoints:\n",
    "        i.undeploy_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternatives for PaliGemma\n",
    "\n",
    "https://ai.google.dev/gemma/docs/paligemma/inference-with-keras"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
